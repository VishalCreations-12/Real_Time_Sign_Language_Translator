{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#CNN-based Sign to text/speech conversion"
      ],
      "metadata": {
        "id": "Yow0CBImZrP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install Necessary Libraries\n",
        "!pip install mediapipe tensorflow gtts opencv-python matplotlib numpy\n",
        "\n",
        "# Step 2: Import Libraries\n",
        "import os\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from gtts import gTTS\n",
        "from IPython.display import Audio, display\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "\n",
        "# Step 3: Initialize MediaPipe\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.5)\n",
        "\n",
        "# Step 4: Define Functions for Keypoint Extraction\n",
        "def extract_keypoints(image):\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    result = hands.process(image_rgb)\n",
        "    if result.multi_hand_landmarks:\n",
        "        keypoints = []\n",
        "        for hand_landmarks in result.multi_hand_landmarks:\n",
        "            for lm in hand_landmarks.landmark:\n",
        "                keypoints.extend([lm.x, lm.y, lm.z])\n",
        "        return keypoints\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Step 5: Load Dataset and Extract Features\n",
        "DATASET_PATH = \"/content/sign_language_dataset1\"  # Update as needed\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "for label in os.listdir(DATASET_PATH):\n",
        "    label_path = os.path.join(DATASET_PATH, label)\n",
        "    if os.path.isdir(label_path):\n",
        "        for image_file in os.listdir(label_path):\n",
        "            image_path = os.path.join(label_path, image_file)\n",
        "            image = cv2.imread(image_path)\n",
        "            keypoints = extract_keypoints(image)\n",
        "            if keypoints:\n",
        "                data.append(keypoints)\n",
        "                labels.append(label)\n",
        "\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Encode labels\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "encoded_labels = encoder.fit_transform(labels)\n",
        "\n",
        "# Step 6: Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, encoded_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 7: Build and Train Deep Learning Model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(data.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(len(np.unique(encoded_labels)), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, validation_split=0.2, epochs=1000, callbacks=[early_stop], batch_size=32)\n",
        "\n",
        "# Plot Training History\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Step 8: Evaluate the Model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Step 9: Save the Model and Encoder\n",
        "model.save('sign_language_model.h5')\n",
        "np.save('label_encoder_classes.npy', encoder.classes_)\n",
        "\n",
        "# Step 10: Functions for Prediction and TTS\n",
        "def predict_sign_language(keypoints):\n",
        "    if keypoints is not None:\n",
        "        keypoints = np.array(keypoints).reshape(1, -1)\n",
        "        prediction = model.predict(keypoints)\n",
        "        predicted_label = np.argmax(prediction)\n",
        "        sign = encoder.inverse_transform([predicted_label])[0]\n",
        "        return sign\n",
        "    return None\n",
        "\n",
        "def text_to_speech(text):\n",
        "    tts = gTTS(text)\n",
        "    tts.save(\"output.mp3\")\n",
        "    display(Audio(\"output.mp3\", autoplay=True))\n",
        "# Function to visualize MediaPipe processed image with landmarks\n",
        "def visualize_hand(image, landmarks):\n",
        "    \"\"\"\n",
        "    Draws hand landmarks on the image and returns it.\n",
        "    \"\"\"\n",
        "    annotated_image = image.copy()\n",
        "    for hand_landmarks in landmarks:\n",
        "        mp_drawing.draw_landmarks(\n",
        "            annotated_image, hand_landmarks, mp_hands.HAND_CONNECTIONS\n",
        "        )\n",
        "    return annotated_image\n",
        "\n",
        "# Update `test_uploaded_image` function to include visualization\n",
        "def test_uploaded_image():\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "        image_path = filename\n",
        "        image = cv2.imread(image_path)\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        result = hands.process(image_rgb)\n",
        "\n",
        "        if result.multi_hand_landmarks:\n",
        "            # Display MediaPipe hand visualization\n",
        "            processed_image = visualize_hand(image, result.multi_hand_landmarks)\n",
        "            plt.imshow(cv2.cvtColor(processed_image, cv2.COLOR_BGR2RGB))\n",
        "            plt.axis('off')\n",
        "            plt.title(\"MediaPipe Processed Image\")\n",
        "            plt.show()\n",
        "\n",
        "            # Extract keypoints\n",
        "            keypoints = []\n",
        "            for hand_landmarks in result.multi_hand_landmarks:\n",
        "                for lm in hand_landmarks.landmark:\n",
        "                    keypoints.extend([lm.x, lm.y, lm.z])\n",
        "\n",
        "            # Predict the gesture\n",
        "            prediction = predict_sign_language(keypoints)\n",
        "            if prediction:\n",
        "                print(f\"Detected Sign: {prediction}\")\n",
        "                text_to_speech(prediction)\n",
        "            else:\n",
        "                print(\"No valid sign detected!\")\n",
        "        else:\n",
        "            print(\"No hand detected!\")\n",
        "\n",
        "# Update `real_time_detection` function to include visualization\n",
        "def real_time_detection():\n",
        "    cap = cv2.VideoCapture(0)\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        result = hands.process(image_rgb)\n",
        "\n",
        "        if result.multi_hand_landmarks:\n",
        "            # Extract keypoints\n",
        "            keypoints = []\n",
        "            for hand_landmarks in result.multi_hand_landmarks:\n",
        "                for lm in hand_landmarks.landmark:\n",
        "                    keypoints.extend([lm.x, lm.y, lm.z])\n",
        "\n",
        "            # Predict the gesture\n",
        "            prediction = predict_sign_language(keypoints)\n",
        "\n",
        "            # Display MediaPipe hand visualization in a separate window\n",
        "            processed_image = visualize_hand(frame, result.multi_hand_landmarks)\n",
        "            cv2.imshow(\"MediaPipe Processed Image\", processed_image)\n",
        "\n",
        "            if prediction:\n",
        "                # Display prediction on the original frame\n",
        "                cv2.putText(frame, f\"Detected: {prediction}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
        "\n",
        "                # Text-to-Speech Conversion\n",
        "                text_to_speech(prediction)\n",
        "\n",
        "        cv2.imshow('Sign Language Detection', frame)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Test Functions\n",
        "# test_uploaded_image()\n",
        "# real_time_detection()"
      ],
      "metadata": {
        "id": "YT130USyUtto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test Functions\n",
        "test_uploaded_image()\n"
      ],
      "metadata": {
        "id": "IFQcYIFQVFHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j34nNHpMva3a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}