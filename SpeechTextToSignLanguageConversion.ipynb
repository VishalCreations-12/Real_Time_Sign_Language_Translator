{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install SpeechRecognition moviepy librosa tensorflow keras\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from moviepy.editor import VideoFileClip\n",
        "from IPython.display import Video, display\n",
        "import speech_recognition as sr\n",
        "from IPython.display import HTML\n",
        "\n",
        "\n",
        "# Set dataset path\n",
        "DATASET_PATH = \"/content/sign_language_dataset2\"\n",
        "\n",
        "# Step 1: Load dataset and prepare labels\n",
        "gesture_videos = {os.path.splitext(f)[0].lower(): os.path.join(DATASET_PATH, f)\n",
        "                  for f in os.listdir(DATASET_PATH) if f.endswith('.mp4')}\n",
        "labels = list(gesture_videos.keys())\n",
        "\n",
        "# Label encoding for gestures\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Step 2: Feature extraction for video (using placeholder features)\n",
        "def extract_video_features(video_path):\n",
        "    # For simplicity, using video length as a feature. Extendable to frame-based features.\n",
        "    clip = VideoFileClip(video_path)\n",
        "    duration = clip.duration\n",
        "    clip.close()\n",
        "    return np.array([duration])\n",
        "\n",
        "# Build dataset features and labels\n",
        "video_features = np.array([extract_video_features(path) for path in gesture_videos.values()])\n",
        "video_labels = np.array(encoded_labels)\n",
        "\n",
        "# Step 3: Train ML/DL model for text/speech to video mapping\n",
        "# Convert labels to one-hot encoding\n",
        "one_hot_labels = tf.keras.utils.to_categorical(video_labels, num_classes=len(labels))\n",
        "\n",
        "# Define a simple LSTM model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=len(labels), output_dim=64, input_length=1),\n",
        "    LSTM(128, return_sequences=True),\n",
        "    Dropout(0.3),\n",
        "    LSTM(64),\n",
        "    Dense(len(labels), activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "model.fit(video_features, one_hot_labels, epochs=1000, batch_size=4)\n",
        "\n",
        "# Step 4: Map input speech/text to video\n",
        "def preprocess_text(input_text):\n",
        "    return re.sub(r'[^a-zA-Z0-9 ]', '', input_text).strip().lower()\n",
        "\n",
        "def predict_video(input_text):\n",
        "    label = preprocess_text(input_text)\n",
        "    if label in labels:\n",
        "        encoded_input = np.array([label_encoder.transform([label])[0]])\n",
        "        prediction = model.predict(encoded_input)\n",
        "        predicted_label = label_encoder.inverse_transform([np.argmax(prediction)])\n",
        "        return gesture_videos.get(predicted_label[0])\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def play_video(video_path):\n",
        "    if video_path:\n",
        "        display(Video(video_path, embed=True))\n",
        "    else:\n",
        "        print(\"No corresponding gesture found.\")\n",
        "\n",
        "\n",
        "# Step 5: Speech-to-text conversion\n",
        "def speech_to_text(audio_path=None):\n",
        "    recognizer = sr.Recognizer()\n",
        "    if audio_path:\n",
        "        with sr.AudioFile(audio_path) as source:\n",
        "            audio_data = recognizer.record(source)\n",
        "    else:\n",
        "        with sr.Microphone() as source:\n",
        "            print(\"Speak now...\")\n",
        "            audio_data = recognizer.listen(source)\n",
        "    try:\n",
        "        return recognizer.recognize_google(audio_data)\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Could not understand audio.\")\n",
        "        return None\n",
        "    except sr.RequestError:\n",
        "        print(\"Service unavailable.\")\n",
        "        return None\n",
        "\n",
        "# Step 6: Full pipeline\n",
        "def convert_to_sign_language(input_text=None, audio_path=None):\n",
        "    if not input_text and not audio_path:\n",
        "        print(\"Please provide either text or an audio input.\")\n",
        "        return\n",
        "    if audio_path:\n",
        "        input_text = speech_to_text(audio_path)\n",
        "    if input_text:\n",
        "        print(f\"Input: {input_text}\")\n",
        "        video_path = predict_video(input_text)\n",
        "        play_video(video_path)\n",
        "\n",
        "    else:\n",
        "        print(\"Unable to process input.\")\n",
        "\n",
        "# Example Usage\n",
        "# Text Input\n",
        "#convert_to_sign_language(input_text=\"hello\")\n",
        "\n",
        "# Speech Input (Use an audio file path or microphone)\n",
        "# convert_to_sign_language(audio_path=\"/path/to/audio.wav\")\n"
      ],
      "metadata": {
        "id": "SCAh6oXPA2Mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert_to_sign_language(input_text=\"alone\")"
      ],
      "metadata": {
        "id": "4dp2Em4hK8GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_sign_language(text):\n",
        "    processed_text = preprocess_text(text)\n",
        "    words = processed_text.split()\n",
        "\n",
        "    print(f\"Processing text: {processed_text}\")\n",
        "    for word in words:\n",
        "        gesture_index = label_map.get(word.lower())\n",
        "        if gesture_index is not None:\n",
        "            video_path = train_videos[gesture_index]\n",
        "            cap = cv2.VideoCapture(video_path)\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                cv2.imshow('Sign Language Gesture', frame)\n",
        "                if cv2.waitKey(30) & 0xFF == ord('q'):  # Press 'q' to quit\n",
        "                    break\n",
        "            cap.release()\n",
        "            cv2.destroyAllWindows()\n",
        "        else:\n",
        "            print(f\"No gesture found for: {word}\")\n"
      ],
      "metadata": {
        "id": "bs1tnnheLMIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def real_time_system():\n",
        "    mode = input(\"Choose mode (1 for Speech, 2 for Text): \")\n",
        "    if mode == \"1\":\n",
        "        # text = speech_to_text()\n",
        "        print(\"still in processing phase\")\n",
        "    elif mode == \"2\":\n",
        "        text = input(\"Enter text: \")\n",
        "    else:\n",
        "        print(\"Invalid mode!\")\n",
        "        return\n",
        "\n",
        "    if text:\n",
        "        convert_to_sign_language(input_text=text)\n"
      ],
      "metadata": {
        "id": "ypcj0UGELNcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_time_system()\n"
      ],
      "metadata": {
        "id": "cOxSHnhoLP-B",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}